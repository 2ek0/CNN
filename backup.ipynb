{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45651c82",
   "metadata": {},
   "source": [
    "## 2. Visualization Functions\n",
    "\n",
    "Functions for visualizing the training data and results:\n",
    "- Plotting sample images with their classes\n",
    "- Plotting confusion matrices\n",
    "- Visualizing example errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e56a6d7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    \"\"\"Plot 9 images in a 3x3 grid with true and predicted classes.\"\"\"\n",
    "    if len(images) == 0:\n",
    "        print(\"no images to show\")\n",
    "        return \n",
    "    else:\n",
    "        random_indices = random.sample(range(len(images)), min(len(images), 9))\n",
    "        \n",
    "    if cls_pred is not None:\n",
    "        images, cls_true, cls_pred = zip(*[(images[i], cls_true[i], cls_pred[i]) for i in random_indices])\n",
    "    else:\n",
    "        images, cls_true = zip(*[(images[i], cls_true[i]) for i in random_indices])\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_size, img_size, num_channels))\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = f\"True: {cls_true[i]}\"\n",
    "        else:\n",
    "            xlabel = f\"True: {cls_true[i]}\\nPred: {cls_pred[i]}\"\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cls_pred, cls_true):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "    # Get the confusion matrix using sklearn.\n",
    "    cm = confusion_matrix(y_true=cls_true,\n",
    "                         y_pred=cls_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Set up the axis labels with just numbers 0-4\n",
    "    num_classes = 5\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks)\n",
    "    plt.yticks(tick_marks)\n",
    "    \n",
    "    # Normalize the confusion matrix.\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Add percentage labels to the plot.\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, '{:.2f}%'.format(cm[i, j]*100),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743d6a5b",
   "metadata": {},
   "source": [
    "## 3. CNN Model Implementation\n",
    "\n",
    "The CNN model consists of:\n",
    "1. Image processing branch with convolutional layers\n",
    "2. Weather feature processing branch with dense layers\n",
    "3. Combined network for final classification\n",
    "\n",
    "Let's implement the helper functions for creating layers first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67cb08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    \"\"\"Create new TensorFlow weights.\"\"\"\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def new_biases(length):\n",
    "    \"\"\"Create new TensorFlow biases.\"\"\"\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))\n",
    "\n",
    "def new_conv_layer(input, num_input_channels, filter_size, num_filters, use_pooling=True):\n",
    "    \"\"\"Create a new convolutional layer.\"\"\"\n",
    "    # Shape of the filter-weights for the convolution.\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Create new weights and biases\n",
    "    weights = new_weights(shape=shape)\n",
    "    biases = new_biases(length=num_filters)\n",
    "\n",
    "    # Create the TensorFlow operation for convolution.\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='SAME')\n",
    "\n",
    "    # Add the biases to the results of the convolution.\n",
    "    layer += biases\n",
    "\n",
    "    # Use pooling to down-sample the image resolution?\n",
    "    if use_pooling:\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                              ksize=[1, 2, 2, 1],\n",
    "                              strides=[1, 2, 2, 1],\n",
    "                              padding='SAME')\n",
    "\n",
    "    # Rectified Linear Unit (ReLU).\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer, weights\n",
    "\n",
    "def flatten_layer(layer):\n",
    "    \"\"\"Flatten a layer for fully-connected layers.\"\"\"\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    return layer_flat, num_features\n",
    "\n",
    "def new_fc_layer(input, num_inputs, num_outputs, use_relu=True):\n",
    "    \"\"\"Create a new fully-connected layer.\"\"\"\n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeea1c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weather_cnn(num_classes):\n",
    "    \"\"\"Create the multi-modal weather classification CNN with both images and weather features.\"\"\"\n",
    "    # Placeholder variables for images\n",
    "    x_images = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x_images')\n",
    "    x_image = tf.reshape(x_images, [-1, img_size, img_size, num_channels])\n",
    "    \n",
    "    # Placeholder variables for weather features\n",
    "    x_weather = tf.placeholder(tf.float32, shape=[None, num_weather_features], name='x_weather')\n",
    "    \n",
    "    y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "    y_true_cls = tf.argmax(y_true, axis=1)\n",
    "\n",
    "    # Convolutional layers for image processing\n",
    "    layer_conv1, weights_conv1 = new_conv_layer(input=x_image,\n",
    "                                               num_input_channels=num_channels,\n",
    "                                               filter_size=filter_size1,\n",
    "                                               num_filters=num_filters1,\n",
    "                                               use_pooling=True)\n",
    "\n",
    "    layer_conv2, weights_conv2 = new_conv_layer(input=layer_conv1,\n",
    "                                               num_input_channels=num_filters1,\n",
    "                                               filter_size=filter_size2,\n",
    "                                               num_filters=num_filters2,\n",
    "                                               use_pooling=True)\n",
    "\n",
    "    layer_conv3, weights_conv3 = new_conv_layer(input=layer_conv2,\n",
    "                                               num_input_channels=num_filters2,\n",
    "                                               filter_size=filter_size3,\n",
    "                                               num_filters=num_filters3,\n",
    "                                               use_pooling=True)\n",
    "\n",
    "    # Flatten the convolutional output\n",
    "    layer_flat, num_features = flatten_layer(layer_conv3)\n",
    "\n",
    "    # Fully-connected layer for image features\n",
    "    layer_fc1_images = new_fc_layer(input=layer_flat,\n",
    "                                   num_inputs=num_features,\n",
    "                                   num_outputs=fc_size,\n",
    "                                   use_relu=True)\n",
    "\n",
    "    # Fully-connected layer for weather features\n",
    "    layer_fc1_weather = new_fc_layer(input=x_weather,\n",
    "                                    num_inputs=num_weather_features,\n",
    "                                    num_outputs=weather_fc_size,\n",
    "                                    use_relu=True)\n",
    "\n",
    "    # Concatenate image and weather features\n",
    "    layer_combined = tf.concat([layer_fc1_images, layer_fc1_weather], axis=1)\n",
    "    combined_size = fc_size + weather_fc_size\n",
    "\n",
    "    # Final fully-connected layer for classification\n",
    "    layer_fc2 = new_fc_layer(input=layer_combined,\n",
    "                            num_inputs=combined_size,\n",
    "                            num_outputs=num_classes,\n",
    "                            use_relu=False)\n",
    "\n",
    "    # Predicted Class\n",
    "    y_pred = tf.nn.softmax(layer_fc2)\n",
    "    y_pred_cls = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Cost-function to be optimized\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=layer_fc2, labels=y_true)\n",
    "    cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # Optimization Method\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "\n",
    "    # Performance Measures\n",
    "    correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    return x_images, x_weather, y_true, cost, accuracy, y_pred_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036243fd",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluation Functions\n",
    "\n",
    "Functions for:\n",
    "- Training the model\n",
    "- Printing progress and validation accuracy\n",
    "- Early stopping based on validation loss\n",
    "- Plotting training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954dd30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(num_iterations, data, session, model_vars, early_stopping=None):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    # Unpack model variables\n",
    "    x_images, x_weather, y_true, cost, accuracy, y_pred_cls = model_vars\n",
    "    \n",
    "    # For plotting learning curve\n",
    "    costs = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    patience = early_stopping if early_stopping else float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"Training started...\")\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Get a batch of training data\n",
    "        x_batch, y_batch, _, cls_batch, features_batch = data.train.next_batch(batch_size)\n",
    "        \n",
    "        # Create feed dictionary for training\n",
    "        feed_dict_train = {\n",
    "            x_images: x_batch,\n",
    "            x_weather: features_batch,\n",
    "            y_true: y_batch\n",
    "        }\n",
    "        \n",
    "        # Create feed dictionary for validation\n",
    "        feed_dict_val = {\n",
    "            x_images: data.valid.images,\n",
    "            x_weather: data.valid.features,\n",
    "            y_true: data.valid.labels\n",
    "        }\n",
    "        \n",
    "        # Run the optimizer using this batch of training data\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            # Calculate the accuracy on the training-batch\n",
    "            acc_train = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "            acc_val = session.run(accuracy, feed_dict=feed_dict_val)\n",
    "            val_loss = session.run(cost, feed_dict=feed_dict_val)\n",
    "            \n",
    "            costs.append(val_loss)\n",
    "            train_accuracies.append(acc_train)\n",
    "            val_accuracies.append(acc_val)\n",
    "            \n",
    "            msg = \"Iteration: {0:>6}, Training Accuracy: {1:>6.1%}, Validation Accuracy: {2:>6.1%}, Validation Loss: {3:.3f}\"\n",
    "            print(msg.format(i, acc_train, acc_val, val_loss))\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "    \n",
    "    # Plot the learning curves\n",
    "    iterations = range(0, num_iterations, 100)[:len(costs)]\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(iterations, costs)\n",
    "    plt.title('Validation Loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(iterations, train_accuracies, label='Training')\n",
    "    plt.plot(iterations, val_accuracies, label='Validation')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return costs, train_accuracies, val_accuracies\n",
    "\n",
    "def print_validation_accuracy(data, session, model_vars, cluster_descriptions):\n",
    "    \"\"\"Calculate and print the validation accuracy.\"\"\"\n",
    "    # Unpack model variables\n",
    "    x_images, x_weather, y_true, cost, accuracy, y_pred_cls = model_vars\n",
    "    \n",
    "    # Create feed-dict for the validation data\n",
    "    feed_dict = {\n",
    "        x_images: data.valid.images,\n",
    "        x_weather: data.valid.features,\n",
    "        y_true: data.valid.labels\n",
    "    }\n",
    "    \n",
    "    # Get the predicted class-numbers\n",
    "    cls_pred = session.run(y_pred_cls, feed_dict=feed_dict)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    correct = (cls_pred == data.valid.cls)\n",
    "    correct_sum = correct.sum()\n",
    "    acc = float(correct_sum) / len(correct)\n",
    "    \n",
    "    print(\"Validation Accuracy: {0:.1%}\".format(acc))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    class_names = [cluster_descriptions[i]['description'] for i in range(len(cluster_descriptions))]\n",
    "    plot_confusion_matrix(cls_pred, data.valid.cls, class_names)\n",
    "    \n",
    "    # Show some example errors\n",
    "    mask = ~correct\n",
    "    incorrect_images = data.valid.images[mask]\n",
    "    incorrect_cls_true = data.valid.cls[mask]\n",
    "    incorrect_cls_pred = cls_pred[mask]\n",
    "    \n",
    "    # Plot some example errors\n",
    "    if len(incorrect_images) > 0:\n",
    "        print(\"\\nExample errors:\")\n",
    "        plot_images(incorrect_images, incorrect_cls_true, incorrect_cls_pred, cluster_descriptions)\n",
    "    else:\n",
    "        print(\"\\nNo errors found in validation set!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
