{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c60132aa",
   "metadata": {},
   "source": [
    "# Multi-Modal Weather Classification CNN\n",
    "\n",
    "This notebook implements a Convolutional Neural Network (CNN) for weather classification using both image data and weather measurements. The system combines:\n",
    "\n",
    "1. Image features from sky images\n",
    "2. Weather measurements (cloud coverage, irradiance, sun obscuration)\n",
    "3. Time-based features (hour, month)\n",
    "\n",
    "The model architecture uses two branches:\n",
    "- CNN branch for processing images\n",
    "- Dense network branch for weather features\n",
    "These branches are then combined for final classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6216a4",
   "metadata": {},
   "source": [
    "## 1. Required Imports and Configuration\n",
    "\n",
    "First, we'll import the required libraries and set up our configuration parameters. For Google Colab, we need to ensure all dependencies are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "455ef59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import json as json\n",
    "import cv2\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Add current directory to path for imports\n",
    "current_dir = os.path.abspath(os.path.dirname(''))\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.append(current_dir)\n",
    "\n",
    "# Import our custom weather dataset with clustering\n",
    "from weather_dataset import (\n",
    "    read_weather_sets_with_clustering,\n",
    "    get_cluster_characteristics,\n",
    "    get_weather_classes_from_clusters,\n",
    "    analyze_weather_distribution\n",
    " )\n",
    "\n",
    "# Configuration and Hyperparameters\n",
    "# Convolutional Layers\n",
    "filter_size1 = 3 \n",
    "num_filters1 = 32\n",
    "filter_size2 = 3\n",
    "num_filters2 = 32\n",
    "filter_size3 = 3\n",
    "num_filters3 = 64\n",
    "\n",
    "# Fully-connected layers\n",
    "fc_size = 128             # Number of neurons in fully-connected layer\n",
    "weather_fc_size = 64      # Number of neurons for weather features\n",
    "\n",
    "# Image configuration\n",
    "num_channels = 3          # RGB images\n",
    "img_size = 224           # image dimensions (square)\n",
    "img_size_flat = img_size * img_size * num_channels\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Weather features configuration\n",
    "num_weather_features = 7  # cloud_coverage, sun_obscuration_percentage, irradiance, hour_sin, hour_cos, month_sin, month_cos\n",
    "n_clusters = 5           # Fixed number of weather classes\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 16\n",
    "early_stopping = 10      # Number of epochs to wait for improvement\n",
    "\n",
    "# Data paths\n",
    "base_dir = r\"D:\\Image\"  # Your image directory\n",
    "test_result_dir = os.path.join(base_dir, \"test_result\")\n",
    "json_dir = os.path.join(test_result_dir, \"json\")\n",
    "checkpoint_dir = os.path.join(test_result_dir, \"models\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c588c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function: Resize and save images to disk\n",
    "def resize_and_save_images(image_paths, resized_dir, size=(224, 224)):\n",
    "    new_paths = []\n",
    "    for img_path in image_paths:\n",
    "        save_path = os.path.join(resized_dir, os.path.basename(img_path))\n",
    "        if os.path.exists(save_path):\n",
    "            # Skip if already resized\n",
    "            new_paths.append(save_path)\n",
    "            continue\n",
    "        img_full_path = os.path.join(base_dir, os.path.basename(img_path))\n",
    "        if not os.path.exists(img_full_path):\n",
    "            print(f\"Image not found: {img_full_path}\")\n",
    "            continue\n",
    "        img = cv2.imread(img_full_path)\n",
    "        if img is None:\n",
    "            print(f\"Could not read image: {img_full_path}\")\n",
    "            continue\n",
    "        img_resized = cv2.resize(img, size, cv2.INTER_LINEAR)\n",
    "        img_resized = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
    "        cv2.imwrite(save_path, cv2.cvtColor(img_resized, cv2.COLOR_RGB2BGR))\n",
    "        new_paths.append(save_path)\n",
    "    return new_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa28fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load resized images from disk and match with features/labels\n",
    "def load_images_from_paths(image_paths, size=(224, 224)):\n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            print(f\"Could not read image: {path}\")\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, size, cv2.INTER_LINEAR)  # Just in case\n",
    "        images.append(img)\n",
    "    return np.array(images, dtype=np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddedf79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation study options: set these before running training\n",
    "# Remove features by listing their names in ablation_features\n",
    "# Options: 'cloud_coverage', 'sun_obscuration_percentage', 'irradiance', 'time'\n",
    "# To remove images, set ablation_no_image = True\n",
    "ablation_features = []  # e.g., ['cloud_coverage', 'irradiance', 'time']\n",
    "ablation_no_image = False  # Set True to train without images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d0ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to TFRecord format for efficient streaming\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def serialize_example(image, features, label):\n",
    "    feature = {\n",
    "        'image': _bytes_feature(image.tobytes()),\n",
    "        'features': _float_feature(features.tolist()),\n",
    "        'label': _float_feature(label.tolist()),\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "def write_tfrecord(images, features, labels, filename):\n",
    "    with tf.io.TFRecordWriter(filename) as writer:\n",
    "        for img, feat, lbl in zip(images, features, labels):\n",
    "            example = serialize_example(img, feat, lbl)\n",
    "            writer.write(example)\n",
    "    print(f\"TFRecord written: {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a63c35c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and use TFRecords in your training pipeline\n",
    "import tensorflow as tf\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    feature_description = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'features': tf.io.FixedLenFeature([X_train_features_resized.shape[1]], tf.float32),\n",
    "        'label': tf.io.FixedLenFeature([Y_train_resized.shape[1]], tf.float32),\n",
    "    }\n",
    "    parsed = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    image = tf.io.decode_raw(parsed['image'], tf.float32)\n",
    "    image = tf.reshape(image, [img_size, img_size, num_channels])\n",
    "    features = parsed['features']\n",
    "    label = parsed['label']\n",
    "    return (image, features), label\n",
    "\n",
    "def get_dataset_from_tfrecord(tfrecord_path, batch_size):\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "    dataset = dataset.map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Example: use train_dataset and val_dataset in model.fit\n",
    "# history = model.fit(train_dataset, validation_data=val_dataset, epochs=30, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc545b9",
   "metadata": {},
   "source": [
    "## 5. Training and Evaluation\n",
    "\n",
    "1. Load and preprocess the data\n",
    "2. Create and train the model\n",
    "3. Evaluate the results\n",
    "4. Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe625fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1] Loading weather dataset paths and features...\n",
      "Reading weather data from JSON files...\n",
      "Found 80 JSON files\n",
      "Performing K-means clustering with 5 clusters...\n",
      "Performing K-means clustering with 5 clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nitro\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering completed. Cluster distribution: [36573  8657 14362  9019 25377]\n",
      "Loaded 93988 images with clustered weather data\n",
      "Weather class distribution: [36573  8657 14362  9019 25377]\n",
      "Loaded 93988 images with clustered weather data\n",
      "Weather class distribution: [36573  8657 14362  9019 25377]\n",
      "[Step 1] Done loading weather dataset paths and features.\n",
      "[Step 1] Done loading weather dataset paths and features.\n",
      "TFRecord written: tfrecords/train.tfrecord\n",
      "TFRecord written: tfrecords/train.tfrecord\n",
      "TFRecord written: tfrecords/val.tfrecord\n",
      "TFRecord written: tfrecords/val.tfrecord\n",
      "TFRecord written: tfrecords/test.tfrecord\n",
      "TFRecord written: tfrecords/test.tfrecord\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load only image paths, features, and labels (not image arrays) to avoid memory errors\n",
    "print(\"[Step 1] Loading weather dataset paths and features...\")\n",
    "data = read_weather_sets_with_clustering(\n",
    "    json_dir=json_dir,\n",
    "    image_dir=base_dir,\n",
    "    image_size=None,  # Do not load images into memory\n",
    "    use_features=True,\n",
    "    n_clusters=n_clusters,\n",
    "    ablation_features=ablation_features,\n",
    "    ablation_no_image=ablation_no_image\n",
    " )\n",
    "print(\"[Step 1] Done loading weather dataset paths and features.\")\n",
    "\n",
    "# Step 2: Load resized image paths from disk (already resized, do not resize again)\n",
    "resized_dir = os.path.join(base_dir, 'resized224')\n",
    "train_resized_paths = [os.path.join(resized_dir, os.path.basename(p)) for p in data.train.ids]\n",
    "val_resized_paths = [os.path.join(resized_dir, os.path.basename(p)) for p in data.valid.ids]\n",
    "test_resized_paths = [os.path.join(resized_dir, os.path.basename(p)) for p in data.test.ids]\n",
    "\n",
    "# Features and labels remain the same order as original ids\n",
    "Y_train_resized = data.train.labels\n",
    "X_train_features_resized = data.train.features\n",
    "Y_val_resized = data.valid.labels\n",
    "X_val_features_resized = data.valid.features\n",
    "Y_test_resized = data.test.labels\n",
    "X_test_features_resized = data.test.features\n",
    "\n",
    "def write_tfrecord_from_paths(image_paths, features, labels, filename, size=(224, 224)):\n",
    "    with tf.io.TFRecordWriter(filename) as writer:\n",
    "        for img_path, feat, lbl in zip(image_paths, features, labels):\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, size, cv2.INTER_LINEAR)\n",
    "            example = serialize_example(img, feat, lbl)\n",
    "            writer.write(example)\n",
    "    print(f\"TFRecord written: {filename}\")\n",
    "\n",
    "# Write train, val, test sets to TFRecord files (streaming, not loading all images)\n",
    "os.makedirs('tfrecords', exist_ok=True)\n",
    "write_tfrecord_from_paths(train_resized_paths, X_train_features_resized, Y_train_resized, 'tfrecords/train.tfrecord')\n",
    "write_tfrecord_from_paths(val_resized_paths, X_val_features_resized, Y_val_resized, 'tfrecords/val.tfrecord')\n",
    "write_tfrecord_from_paths(test_resized_paths, X_test_features_resized, Y_test_resized, 'tfrecords/test.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfc2f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 2.x and Keras Training (no tf.Session)\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Use tf.data datasets for training\n",
    "train_dataset = get_dataset_from_tfrecord('tfrecords/train.tfrecord', batch_size)\n",
    "val_dataset = get_dataset_from_tfrecord('tfrecords/val.tfrecord', batch_size)\n",
    "test_dataset = get_dataset_from_tfrecord('tfrecords/test.tfrecord', batch_size)\n",
    "\n",
    "# Image branch\n",
    "image_input = layers.Input(shape=(img_size, img_size, num_channels))\n",
    "x = layers.Conv2D(num_filters1, filter_size1, activation='relu', padding='same')(image_input)\n",
    "x = layers.MaxPooling2D()(x)\n",
    "x = layers.Conv2D(num_filters2, filter_size2, activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D()(x)\n",
    "x = layers.Conv2D(num_filters3, filter_size3, activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(fc_size, activation='relu')(x)\n",
    "\n",
    "# Weather branch\n",
    "weather_input = layers.Input(shape=(num_weather_features,))\n",
    "w = layers.Dense(weather_fc_size, activation='relu')(weather_input)\n",
    "\n",
    "# Combine branches\n",
    "combined = layers.Concatenate()([x, w])\n",
    "output = layers.Dense(n_clusters, activation='softmax')(combined)\n",
    "\n",
    "# Build and compile model\n",
    "model = models.Model(inputs=[image_input, weather_input], outputs=output)\n",
    "model.compile(optimizer=optimizers.Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(\"best_model_all.keras\", save_best_only=True)\n",
    " ]\n",
    "\n",
    "# Train the model using tf.data datasets\n",
    "print(\"[Step 7] Training the model with Keras (using tf.data streaming)...\")\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=30, callbacks=callbacks)\n",
    "with open('training_history_all.json', 'w') as f:\n",
    "    json.dump(history.history, f)\n",
    "print(\"Training history saved to training_history_img.json\")\n",
    "print(\"[Step 7] Model training complete.\")\n",
    "\n",
    "# Evaluate and save\n",
    "val_loss, val_acc = model.evaluate(val_dataset)\n",
    "print(f\"Validation Accuracy: {val_acc*100:.4f}%\")\n",
    "model.save(os.path.join(checkpoint_dir, \"weather_cnn_model_all.h5\"))\n",
    "model.save(os.path.join(checkpoint_dir, \"weather_cnn_model_all.keras\"))\n",
    "print(f\"Model saved to {os.path.join(checkpoint_dir, 'weather_cnn_model_all.h5')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92de9bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNetB0: Train model using tf.data streaming\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout\n",
    "\n",
    "# Use tf.data datasets for training\n",
    "train_dataset = get_dataset_from_tfrecord('tfrecords/train.tfrecord', batch_size)\n",
    "val_dataset = get_dataset_from_tfrecord('tfrecords/val.tfrecord', batch_size)\n",
    "test_dataset = get_dataset_from_tfrecord('tfrecords/test.tfrecord', batch_size)\n",
    "\n",
    "# EfficientNetB0 image branch\n",
    "efficientnet_input = layers.Input(shape=(img_size, img_size, num_channels))\n",
    "base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=efficientnet_input)\n",
    "base_model.trainable = False  # Freeze base model for transfer learning\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = layers.Dense(fc_size, activation='relu')(x)\n",
    "\n",
    "# Weather branch (same as before)\n",
    "weather_input = layers.Input(shape=(num_weather_features,))\n",
    "w = layers.Dense(weather_fc_size, activation='relu')(weather_input)\n",
    "\n",
    "# Combine branches\n",
    "combined = layers.Concatenate()([x, w])\n",
    "output = layers.Dense(n_clusters, activation='softmax')(combined)\n",
    "\n",
    "# Build and compile model\n",
    "efficientnet_model = models.Model(inputs=[efficientnet_input, weather_input], outputs=output)\n",
    "efficientnet_model.compile(optimizer=optimizers.Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "callbacks_efficientnet = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(\"best_model_efficientnet.keras\", save_best_only=True)\n",
    " ]\n",
    "\n",
    "print(\"[Step 7B] Training the model with EfficientNetB0 (using tf.data streaming)...\")\n",
    "history_efficientnet = efficientnet_model.fit(train_dataset, validation_data=val_dataset, epochs=30, callbacks=callbacks_efficientnet)\n",
    "with open('training_history_efficientnet.json', 'w') as f:\n",
    "    json.dump(history_efficientnet.history, f)\n",
    "print(\"Training history saved to training_history_efficientnet.json\")\n",
    "print(\"[Step 7B] EfficientNetB0 model training complete.\")\n",
    "\n",
    "# Evaluate and save\n",
    "val_loss, val_acc = efficientnet_model.evaluate(val_dataset)\n",
    "print(f\"Validation Accuracy: {val_acc*100:.4f}%\")\n",
    "efficientnet_model.save(os.path.join(checkpoint_dir, \"weather_cnn_model_efficientnet.h5\"))\n",
    "efficientnet_model.save(os.path.join(checkpoint_dir, \"weather_cnn_model_efficientnet.keras\"))\n",
    "print(f\"Model saved to {os.path.join(checkpoint_dir, 'weather_cnn_model_efficientnet.h5')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d51b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using MobileNetV3Small pretrained model (using tf.data streaming)\n",
    "from tensorflow.keras.applications import MobileNetV3Small\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout\n",
    "\n",
    "# Use tf.data datasets for training\n",
    "train_dataset = get_dataset_from_tfrecord('tfrecords/train.tfrecord', batch_size)\n",
    "val_dataset = get_dataset_from_tfrecord('tfrecords/val.tfrecord', batch_size)\n",
    "test_dataset = get_dataset_from_tfrecord('tfrecords/test.tfrecord', batch_size)\n",
    "\n",
    "# MobileNetV3Small image branch\n",
    "mobilenet_input = layers.Input(shape=(img_size, img_size, num_channels))\n",
    "base_model_mobilenet = MobileNetV3Small(include_top=False, weights='imagenet', input_tensor=mobilenet_input)\n",
    "base_model_mobilenet.trainable = False\n",
    "x = base_model_mobilenet.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = layers.Dense(fc_size, activation='relu')(x)\n",
    "\n",
    "# Weather branch (same as before)\n",
    "weather_input = layers.Input(shape=(num_weather_features,))\n",
    "w = layers.Dense(weather_fc_size, activation='relu')(weather_input)\n",
    "\n",
    "# Combine branches\n",
    "combined = layers.Concatenate()([x, w])\n",
    "output = layers.Dense(n_clusters, activation='softmax')(combined)\n",
    "\n",
    "# Build and compile model\n",
    "mobilenet_model = models.Model(inputs=[mobilenet_input, weather_input], outputs=output)\n",
    "mobilenet_model.compile(optimizer=optimizers.Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "callbacks_mobilenet = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(\"best_model_mobilenet.keras\", save_best_only=True)\n",
    " ]\n",
    "\n",
    "print(\"[Step 7C] Training the model with MobileNetV3Small (using tf.data streaming)...\")\n",
    "history_mobilenet = mobilenet_model.fit(train_dataset, validation_data=val_dataset, epochs=30, callbacks=callbacks_mobilenet)\n",
    "with open('training_history_mobilenet.json', 'w') as f:\n",
    "    json.dump(history_mobilenet.history, f)\n",
    "print(\"Training history saved to training_history_mobilenet.json\")\n",
    "print(\"[Step 7C] MobileNetV3Small model training complete.\")\n",
    "\n",
    "# Evaluate and save\n",
    "val_loss, val_acc = mobilenet_model.evaluate(val_dataset)\n",
    "print(f\"Validation Accuracy: {val_acc*100:.4f}%\")\n",
    "mobilenet_model.save(os.path.join(checkpoint_dir, \"weather_cnn_model_mobilenet.h5\"))\n",
    "mobilenet_model.save(os.path.join(checkpoint_dir, \"weather_cnn_model_mobilenet.keras\"))\n",
    "print(f\"Model saved to {os.path.join(checkpoint_dir, 'weather_cnn_model_mobilenet.h5')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d32433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Final test evaluation\n",
    "test_loss, test_acc = model.evaluate([X_test_images, X_test_features], Y_test)\n",
    "print(f\"Test Accuracy: {test_acc*100:.4f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Test set classification report\n",
    "y_test_pred = model.predict([X_test_images, X_test_features])\n",
    "y_test_pred_classes = np.argmax(y_test_pred, axis=1)\n",
    "y_test_true_classes = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_true_classes, y_test_pred_classes)\n",
    "precision = precision_score(y_test_true_classes, y_test_pred_classes, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test_true_classes, y_test_pred_classes, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_test_true_classes, y_test_pred_classes, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100:.4f}%\")\n",
    "print(f\"Precision: {precision*100:.4f}%\")\n",
    "print(f\"Recall: {recall*100:.4f}%\")\n",
    "print(f\"F1-score: {f1*100:.4f}%\")\n",
    "\n",
    "print(classification_report(y_test_true_classes, y_test_pred_classes, target_names=[str(c) for c in range(n_clusters)], zero_division=0))\n",
    "\n",
    "# Plot confusion matrix for test set\n",
    "cm = confusion_matrix(y_test_true_classes, y_test_pred_classes)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(n_clusters), yticklabels=range(n_clusters))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Test Set Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Plot collision heatmap (misclassification matrix)\n",
    "collision_matrix = np.zeros_like(cm)\n",
    "for i in range(n_clusters):\n",
    "    for j in range(n_clusters):\n",
    "        if i != j:\n",
    "            collision_matrix[i, j] = cm[i, j]\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(collision_matrix, annot=True, fmt='d', cmap='Reds', xticklabels=range(n_clusters), yticklabels=range(n_clusters))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Collision Heatmap (Misclassifications)\")\n",
    "plt.show()\n",
    "\n",
    "# Show example errors (misclassified images) from test set\n",
    "num_examples = 9\n",
    "incorrect_mask = y_test_pred_classes != y_test_true_classes\n",
    "incorrect_images = X_test_images[incorrect_mask]\n",
    "incorrect_true = y_test_true_classes[incorrect_mask]\n",
    "incorrect_pred = y_test_pred_classes[incorrect_mask]\n",
    "\n",
    "if len(incorrect_images) > 0:\n",
    "    print(\"\\nExample errors from test set:\")\n",
    "    idxs = np.random.choice(len(incorrect_images), min(num_examples, len(incorrect_images)), replace=False)\n",
    "    fig, axes = plt.subplots(1, len(idxs), figsize=(15, 3))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(incorrect_images[idxs[i]].reshape(img_size, img_size, num_channels))\n",
    "        ax.set_title(f\"True: {incorrect_true[idxs[i]]}\\nPred: {incorrect_pred[idxs[i]]}\")\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No errors found in test set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78de5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training history from JSON file\n",
    "with open('training_history_efficientnet.json', 'r') as f:\n",
    "    history = json.load(f)\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Model Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history['loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Model Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28cf28ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"stem_conv\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (None, 225, 225, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Choose which model to load for testing\u001b[39;00m\n\u001b[0;32m      9\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mablation/efficientnet/best_model_efficientnet.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Use ablation EfficientNet model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Load test data from TFRecord\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nitro\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:189\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    186\u001b[0m         is_keras_zip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_keras_zip \u001b[38;5;129;01mor\u001b[39;00m is_keras_dir \u001b[38;5;129;01mor\u001b[39;00m is_hf:\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[0;32m    197\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[0;32m    198\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Nitro\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:365\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    361\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filename: expected a `.keras` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m     )\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_model_from_fileobj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nitro\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:442\u001b[0m, in \u001b[0;36m_load_model_from_fileobj\u001b[1;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zf\u001b[38;5;241m.\u001b[39mopen(_CONFIG_FILENAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    440\u001b[0m     config_json \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 442\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_model_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[0;32m    447\u001b[0m extract_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nitro\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:431\u001b[0m, in \u001b[0;36m_model_from_config\u001b[1;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[1;32m--> 431\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\Nitro\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:733\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 733\u001b[0m         instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    735\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    736\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m could not be deserialized properly. Please\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    737\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ensure that components that are Python object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    741\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    742\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Nitro\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\model.py:660\u001b[0m, in \u001b[0;36mModel.from_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_functional_config \u001b[38;5;129;01mand\u001b[39;00m revivable_as_functional:\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;66;03m# Revive Functional model\u001b[39;00m\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;66;03m# (but not Functional subclasses with a custom __init__)\u001b[39;00m\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional_from_config\n\u001b[1;32m--> 660\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctional_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;66;03m# Either the model has a custom __init__, or the config\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;66;03m# does not contain all the information necessary to\u001b[39;00m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;66;03m# revive a Functional model. This happens when the user creates\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;66;03m# In this case, we fall back to provide all config into the\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;66;03m# constructor of the class.\u001b[39;00m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Nitro\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:577\u001b[0m, in \u001b[0;36mfunctional_from_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    575\u001b[0m node_data \u001b[38;5;241m=\u001b[39m node_data_list[node_index]\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 577\u001b[0m     \u001b[43mprocess_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;66;03m# If the node does not have all inbound layers\u001b[39;00m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;66;03m# available, stop processing and continue later\u001b[39;00m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Nitro\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:507\u001b[0m, in \u001b[0;36mfunctional_from_config.<locals>.process_node\u001b[1;34m(layer, node_data)\u001b[0m\n\u001b[0;32m    504\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m deserialize_node(node_data, created_layers)\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# Call layer on its inputs, thus creating the node\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# and building the layer if needed.\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nitro\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Nitro\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    224\u001b[0m             value,\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    226\u001b[0m         }:\n\u001b[1;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m             )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"stem_conv\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (None, 225, 225, 1)"
     ]
    }
   ],
   "source": [
    "# Load model, read test data, evaluate and visualize results\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Choose which model to load for testing\n",
    "model_path = \"ablation/efficientnet/best_model_efficientnet.keras\"  # Use ablation EfficientNet model\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "print(f\"Loaded model from {model_path}\")\n",
    "\n",
    "# Load test data from TFRecord\n",
    "test_dataset = get_dataset_from_tfrecord('tfrecords/test.tfrecord', batch_size)\n",
    "\n",
    "# Get predictions and true labels from test_dataset\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for (img, features), label in test_dataset:\n",
    "    preds = model.predict([img, features])\n",
    "    y_pred.extend(np.argmax(preds, axis=1))\n",
    "    y_true.extend(np.argmax(label.numpy(), axis=1))\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100:.4f}%\")\n",
    "print(f\"Precision: {precision*100:.4f}%\")\n",
    "print(f\"Recall: {recall*100:.4f}%\")\n",
    "print(f\"F1-score: {f1*100:.4f}%\")\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=[str(c) for c in range(n_clusters)], zero_division=0))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(n_clusters), yticklabels=range(n_clusters))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Test Set Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Collision heatmap (misclassification matrix)\n",
    "collision_matrix = np.zeros_like(cm)\n",
    "for i in range(n_clusters):\n",
    "    for j in range(n_clusters):\n",
    "        if i != j:\n",
    "            collision_matrix[i, j] = cm[i, j]\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(collision_matrix, annot=True, fmt='d', cmap='Reds', xticklabels=range(n_clusters), yticklabels=range(n_clusters))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Collision Heatmap (Misclassifications)\")\n",
    "plt.show()\n",
    "\n",
    "# Show example errors (misclassified images) from test set\n",
    "num_examples = 9\n",
    "incorrect_mask = np.array(y_pred) != np.array(y_true)\n",
    "incorrect_imgs = []\n",
    "incorrect_true = []\n",
    "incorrect_pred = []\n",
    "for (img, features), label in test_dataset:\n",
    "    preds = model.predict([img, features])\n",
    "    pred_classes = np.argmax(preds, axis=1)\n",
    "    true_classes = np.argmax(label.numpy(), axis=1)\n",
    "    for i in range(len(img)):\n",
    "        if pred_classes[i] != true_classes[i]:\n",
    "            incorrect_imgs.append(img[i].numpy())\n",
    "            incorrect_true.append(true_classes[i])\n",
    "            incorrect_pred.append(pred_classes[i])\n",
    "if len(incorrect_imgs) > 0:\n",
    "    print(\"\\nExample errors from test set:\")\n",
    "    idxs = np.random.choice(len(incorrect_imgs), min(num_examples, len(incorrect_imgs)), replace=False)\n",
    "    fig, axes = plt.subplots(1, len(idxs), figsize=(15, 3))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(incorrect_imgs[idxs[i]].reshape(img_size, img_size, num_channels))\n",
    "        ax.set_title(f\"True: {incorrect_true[idxs[i]]}\\nPred: {incorrect_pred[idxs[i]]}\")\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No errors found in test set!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
